# 대규모 시스템 키-값 저장소 장애 처리

---

## 시스템 아키텍처 다이어그램
키-값 저장소를 만드는 데 필요한 다양한 기술적 고려사항들을 살펴보았으니, 이제 아키텍처 다이어그램을 볼건데,
아키텍처의 주요 기능을 먼저 살펴보자면 아래와 같습니다.
- 클라이언트는 키-값 저장소가 제공하는 두 가지 API(`get`, `put`)와 통신한다.
  - `get(key)`
  - `put(key, value)`
- 중재자(coordinator)는 클라이언트에게 키-값 저장소에 대한 프록시(proxy) 역할을 하는 노드다.
  - `proxy`: 중간에서 요청을 대신 처리하는 대리자
- 노드들은 '안정 해시'의 `해시 링` 위에 분포한다.
- 노드를 자동으로 추가 또는 삭제할 수 있도록, 시스템은 완전히 분산된다.
- 데이터는 여러 노드에 다중화된다.(장애 방지를 위해)
- 모든 노드가 같은 책임을 지므로 `SPOF(Single Point of Failure)`는 존재하지 않는다.

![image_6-17.png](image%2Fimage_6-17.png)

> 💡 질문 1.
>
> '모든 노드가 같은 책임을 지므로, SPOF는 존재하지 않는다.'가 이해가 되지 않습니다.
> 
> 앞에서 장애 방지 및 가용성을 위해 비동기 다중화를 한다고 했는데, 시계 방향으로 N개의 노드에 대해서만 다중화를 한다고 했던 것 같습니다.
> 그리고 그 뒷 내용에서, 읽기/쓰기 정족수를 1개만 두는 경우도 있던 것 같은데,
> 
> W=1 인 경우, 즉, '1번 노드에 쓰기 완료! 하자마자 1번 노드에 장애가 발생하면, SPOF가 발생하는 것이 아닌가?' 생각이 들었습니다.
> 
> 🙂 답변
> 
> W=1인 상황에서, 3개의 노드 중 단 하나의 노드에만 쓰기가 성공했고, 운이 없게도 해당 노드에 장애가 발생해서
> 클라이언트가 원하는 데이터를 조회해오지 못할 수 있다고 생각합니다. 그래도 하온님께서 답변해주신 바와 같이, SPOF 라고 보기에는 어려울 것 같다.
> 라는 생각입니다.

> 💡 질문 2.
> 
>  노드들은 물리적으로 분산되어있는 것 같은데 노드간의 통신방식이 궁금합니다.
> 
> 🙂 답변
> 
> 노드 간 발생할 수 있는 통신은 아래 2가지 정도가 있는 것 같습니다.
> - 서로의 상태를 주고 받기
>   - 보통 서로의 상태를 주고 받기 위해선 책에서 소개한 '가십 프로토콜'을 통해 이루어지는 것 같습니다.
> - 서로의 데이터를 읽거나/쓰거나
>   - TCP 기반 프로토콜 통신을 통해 이루어집니다.(카산드라와 레디스도 TCP 기반 자체 개발 프로토콜을 활용한다고 합니다.)
>
> 만약 질문해주신 내용이 '서로 다른 데이터 센터에 분산되어 있는 노드들은 서로의 위치를 어떻게 알고 통신할 수 있는 것일까?' 시라면,
> 키-값 스토리지를 구성 시 설정 파일(e.g `cassandra.yml`)에 클러스터의 이름, 노드들의 ip, port 등의 정보가 필요한 것 같습니다.

---
## 잠깐, 아키텍처 파트를 준비하면서 생긴 궁금증..
### 중재자(Coordinator)는 어떻게 정해질까..
위에서 설명하기를, 중재자란 '클라이언트에게 키-값 저장소(e.g 카산드라)에 대한 프록시(대리자) 역할을 하는 노드'라고 설명 드렸습니다.
이는 곧, '클라이언트(e.g 백엔드 애플리케이션)의 요청을 받는 노드' 라고 할 수 있지 않을까 싶습니다.

Redis 같은 경우, 마스터-슬레이브 구조로, 마스터 노드가 중재자 역할을 하지 않을까 싶고, 카산드라 같은 경우는 모든 노드가 '중재자 노드'가 될 수 있다고 합니다.

카산드라의 경우 클라이언트가 컨텍할 수 있는 지점을 여러 개 작성할 수 있는데, 성공할 때까지 다음 노드로 요청을 보내게 됩니다.
```yaml
contact-points:
  - 192.168.1.10:9042
  - 192.168.1.11:9042
  - 192.168.1.12:9042
```

> 유의할 점은, contact-points 라고 해서, '이번 요청을 받은 노드가 중재자 노드가 되는가?' 는 확실치 않습니다.
> 그래도 제 생각에는 해당 요청에 대해서만 '일시적 중재자' 역할을 수행하지 않을까 싶습니다.

참고) 아래 설정은 우선시하고 싶은 데이터 센터를 지정하는 설정이라고 합니다.
```yml
datastax-java-driver.basic.load-balancing-policy.local-datacenter: kr-central
```

### 카산드라의 시드 노드(Seed Node)란?
분산되어 있는 노드들은 '가십 프로토콜'을 통해 서로의 상태를 주고 받습니다. '가십 프로토콜'을 수행하기 위해서는, 서로의 정보를 알아야합니다.

만약 새로운 노드가 클러스터에 새롭게 추가된다면, 그 노드는 현재 클러스터에 어떤 노드들이 어떤 상태로 존재하는 지 모르게 됩니다.

'시드 노드'는 이러한 정보를 새 노드에게 알려주는 노드입니다.
- 시드 노드: 주번
- 담임 선생님: 개발자
- 새롭게 클러스터에 합류한 노드: 전학온 친구
> 학교 주번과는 다르게 시드 노드는 여러개가 될 수 있습니다.. 제가 이해한 방법이지만, 도움이 되실 지 모르겠네요. 참고만 해주세요.
> 추가로, 모든 노드를 시드 노드로 하면 성능 상 비효율적이라고 합니다.

---

## 완전히 분산된 설계
완전히 분산된 설계를 채택하였으므로, 모든 노드는 아래의 기능들을 지원해야한다고 합니다.
- 클라이언트 API 수신
- 각 노드가 서로의 장애 감지
- 데이터 충돌 해소
- 장애 복구 메커니즘
- 다중화
- 저장소 엔진(e.g SSTable)

---
## 쓰기 경로
쓰기 요청이 특정 노드에 전달되면 무슨 일이 벌어질까요?
(아래 구조는 카산드라의 사례입니다.)
![image_6-19.png](image%2Fimage_6-19.png)

1. 쓰기 요청이 커밋 로그(commit log)파일에 기록된다.
2. 데이터가 메모리 캐시(MemTable)에 기록된다.
3. 메모리 캐시가 가득차거나 사전에 정의된 임계치에 도달하면, 디스크에 있는 SSTable로 flush 한다.
> SSTable: Sorted-String Table 로, <K, V>의 순서쌍을 정렬된 리스트 형태로 관리하는 테이블

### Commit log 파일에 기록하는 이유는 무엇일까?
결론부터 말하자면, '장애 복구'를 위함입니다. 

위에서 설명하기를, 메모리 캐시가 가득차면 그 때서야, 디스크에 있는 SSTable로 `flush`를 한다고 했습니다.

하지만 flush가 발생하기 전에 장애가 발생하면 캐시는 사라지고, 디스크에도 아무런 정보가 남지 않게됩니다.
따라서, 캐시에 기록하기 전에, 디스크에 존재하는 `Commit log`에 기록하는 것입니다.

### '그럼 SSTable은 왜 있는거야?'
SSTable이 데이터를 영구적으로 저장하기 위한 목적을 갖고 있다면 Commit Log만으로도 충분할 것 같은데,
SSTable은 왜 필요한 것일까요?

일단, Commit Log는 단순히 로그 역할만 하기 때문에, 조회에 대한 성능이 최악입니다. 즉, 데이터 저장/조회 성능을 위한 구조가 아닙니다.
> 다만, Commit log는 낙서장처럼 데이터를 순차적으로 추가하기만 하기 때문에 쓰기는 빠릅니다.

반면에, `SSTable`은 정렬된 읽기 효율적인 영구 저장소입니다. 아래에 `SSTable`의 특징을 간략하게 보겠습니다.
- 데이터를 정렬된 상태로, 불변하게 저장
- 조회 시 효율적인 구조(Binary Search, Bloom Filter, Index block 등과 결합된 구조)
- compaction(압축)을 통한 공간 정리

따라서 `SSTable`은 낙서장처럼 막 적지 않고, 이쁘게 작성하기 때문에 쓰는 것은 느리지만, 데이터를 조회하는 것은 빠릅니다.

> 💡 Bloom Filter는 무엇인가요?
> 
> 여기서는 '어느 SSTable에 찾는 키가 있는 지 효율적으로 알아내도록 하는 애' 정도로만 이해하고 아래에서 자세하게 다루겠습니다.

---
## 읽기 경로
방금까지는 쓰기 요청이 특정 노드에 전달되면 무슨 일이 벌어지는 지. 즉, 쓰기 경로에 대해 알아봤습니다.

읽기 요청을 받은 노드는 어떻게 될까요?

읽기 요청을 받은 노드는 데이터가 메모리 캐시에 있는지부터 살핍니다. 메모리 캐시에 데이터가 없다면, `SSTable(디스크)`에서 가져와야합니다.
![image_6-20.png](image%2Fimage_6-20.png)
> 위에서 설명했듯, 어떤 `SSTable(디스크)`에 찾고자 하는 데이터의 키가 있는 지 효율적으로 찾기 위해 `Blomm Filter`가 사용됩니다.

---
## SSTable과 Bloom Filter
DB는 일반적으로 RDBMS와 NoSQL로 나뉩니다. `SSTable`은 NoSQL에서 주로 사용합니다.

`SSTable`은 Sorted-Strings Table의 약어이고, 정렬된 key-value 쌍을 담은 immutable 파일입니다. 정렬이 되어 있기에, 읽기엔 빠르고 불변이기에 쓰기(업데이트/삭제)엔 느립니다.(모든 데이터를 다시 써야하므로)

> `SSTable`을 공부할 때 `LSM Tree`를 같이 알고 가면 좋은데, `LSM Tree` 또한 NoSQL에서 성능을 위해 주로 사용되는 핵심 저장 구조입니다.
> 주로 쓰기 지향적인 NoSQL에서 채택되는 듯 합니다.

### LSM Tree(Log-Structured Merge Tree)
LSM Tree의 동작 흐름은 아래와 같습니다.
- Commit Log에 쓰기
- MemTable에 데이터 저장
- Flush(MemTable이 꽉차면 디스크로 SSTable를 생성)
- 삭제 요청의 경우 MemTable의 해당 키에 tombstone(무덤 표시) 추가
  - 이렇게 하는 이유는 SSTable은 불변이기 때문
- Compaction(압축)
  - SSTable이 너무 많아지면, 병합(Merge) 후, 불필요한 버전 제거
  - tombstone(무덤 표시)를 보고 의미 없는 데이터를 삭제
- 읽기
  - MemTable(캐시) -> SSTable 순으로 탐색
  - Bloom Filter로 SSTable 존재 여부 빠르게 확인

위의 내용을 보면, '쓰기 경로' 목차에서 다뤘던 내용 그대로 입니다. 
> ✨ LSM Tree 핵심 요약
> 
> LSM Tree는 '쓰기 성능을 극대화하기 위해 데이터를 메모리에 모아두었다가 디스크에 순차적으로 저장하고, 주기적으로 병합(compaction)하는 구조'입니다.
> 
> 책에서의 참고문헌[9]에서는 구글이 개발한 키-값 스토리지인 LevelDB에서, 쓰기가 느린 SSTable의 단점을 LSM Tree를 통해 해결했다고 합니다.



### Bloom Filter
`Bloom Fiiter`는 데이터 블록에 특정 키의 데이터가 존재하는 지 확인할 수 있는 '확률적 자료 구조'입니다.

블룸 필터는 쓰기를 할 때, 키에 k개의 해시 함수를 수행합니다. 따라서 키마다 k개의 해시 값을 얻습니다.
아래 사진처럼, 해당 해시 값에 해당하는 `'블룸 필터 배열 칸'을 0에서 1로 변경`합니다.
![bloomfilter.png](image%2Fbloomfilter.png)

다음 번에 키를 조회하고자 할 때, 키에 또 k개의 해시 함수를 수행하여, 해시 값을 도출하고, 해당 해시 값이 모두 1인 데이터 블록이 있는 지 찾습니다.
> 해시 값에 매칭되는 배열 칸이 모두 1이라면, 해당 키가 존재하는 데이터 블록임을 알 수 있습니다.

위에 해싱 함수 메커니즘을 통해 벌써 알아채셨을 지도 모르지만, `Bloom Filter`는 아래와 같은 특징이 있습니다.
- `True Negative`가 절대로 발생하지 않는다.
- `False Positive`의 경우엔 종종 발생할 수 있다.
  
> `True Negative`: 데이터베이스에 존재하는 데이터를 존재하지 않는다 판단하는 것
> 
> `False Positive`: 존재하지 않는 데이터를 존재한다 판단하는 것

## 6장 키-값 저장소 설계 오약
![6장요약.png](image%2F6%EC%9E%A5%EC%9A%94%EC%95%BD.png)





