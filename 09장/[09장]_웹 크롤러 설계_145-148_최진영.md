# 2단계 개략적 설계안 제시 및 동의 구하기
요구사항이 분명해지면 개략적 설계를 진행해야합니다. 아래 그림을 보겠습니다.
> 아래 그림은 참고문헌[4][5] (웹 크롤러에 관한 선행연구)를 참고한 설계안이라고 합니다.
> 
> 앞서 나온, 좋은 웹 크롤러의 속성을 만족하는 설계안 정도로만 이해해주시면 될 듯 합니다.

![image_9-2.png](image%2Fimage_9-2.png)
이 발표에서는 위 다이어그램에 등장하는 각 컴포넌트의 기능에 대해 얘기해보고자 합니다.

### 시작 URL 집합
`시작 URL 집합`은 웹 크롤러가 크롤링을 시작하는 출발점입니다. 예를 들어, 어떤 대학 웹사이트로부터 찾아 나갈 수 있는 모든 웹 페이지를
크롤링하는 가장 직관적인 방법은 해당 대학의 도메인 이름이 붙은 모든 페이지의 URL을 `시작 URL`로 쓰는 것입니다.
```markdown
https://www.dankook.ac.kr/블라블라1
https://www.dankook.ac.kr/블라블라2
https://www.dankook.ac.kr/블라블라3
```

만약 '대학 웹사이트로부터 찾아 나갈 수 있는 사이트들' 이 아닌, '전체 웹'을 크롤링해야하는 경우에는 어떻게 해야할까요?

`시작 URL`을 고를 때 좀 더 창의적일 필요가 있습니다. 크롤러가 가능한 한 많은 링크를 탐색할 수 있어야하기 때문입니다.

일반적으로는 전체 URL을 [지역적 특색, 주제별]과 같이 부분집합으로 나누는 전략을 사용합니다.
- 지역적 특색: 나라별로 인기 있는 웹사이트가 다르다는 점에서 착안
- 주제별 예시) 쇼핑, 스포츠, 건강 등의 주제로 세분화 후 각각에 다른 시작 URL을 사용

> 시작 URL로 무엇을 쓸 것이냐는 정답이 없다!

### 미수집 URL 저장소
대부분의 현대적 웹 크롤러는 크롤링 상태를 아래 두 가지 상태로 나눠 관리합니다.
- 다운로드할 URL
- 다운로드된 URL

이 중, `다운로드할 URL`을 저장하고 관리하는 컴포넌트를 `미수집 URL 저장소(URL frontier)` 라고 부릅니다. FIFO 큐라고 생각하면 됩니다.
> 🎮 frontier는 미개척지라는 의미가 있습니다. 즉, 아직 크롤러가 수집하지 않은 URL들로 이해하시면 되겠습니다.

### HTML 다운로더
`HTML 다운로더`는 인터넷에서 웹 페이지를 다운로드하는 컴포넌트입니다. 다운로드할 페이지의 URL은 `미수집 URL 저장소`가 제공합니다.

### 도메인 이름 변환기
웹 페이지를 다운로드하려면 URL을 IP 주소료 변환하는 절차가 필요합니다. 따라서 `HTML 다운로더`는 `도메인 이름 변환기`를 사용하여
URL에 대응되는 IP주소를 알아냅니다.
> 왜 URL을 IP주소로 변환하는 절차가 필요한가요?
> 
> URL은 사람이 이해하기 쉽도록 만든 이름입니다. 결국 크롤러가 웹사이트에 요청을 보내기 위해서는 IP주소로 통신해야합니다.

### 콘텐츠 파서
웹 페이지를 다운로드하면 `파싱(parsing)`과 `검증(validation)`절차를 거처야 합니다. 이상한 웹 페이지는 문제를 일으킬 수 있는데다,
저장 공간만 낭비하게 되기 때문입니다.

크롤링 서버 안에 `콘텐츠 파서`를 구현하면 크롤링 과정이 느려지게 될 수 있기 때문에 독립된 컴포넌트로 만들었다고 합니다.

### 중복 컨텐츠인가?
웹에 공개된 연구 결과에 따르면, 29% 가량의 웹 페이지 콘텐츠는 중복이라고 합니다. 따라서 같은 콘텐츠를 여러 번 저장하게 될 수 있습니다.
> 본 설계안의 경우, 이를 해결하기 위한 자료 구조를 도입하여 데이터 중복을 줄이고 데이터 처리에 소요되는 시간을 줄인다고 합니다.
> 
> 이미 시스템에 저장된 콘텐츠임을 쉽게 알아내게끔 하는 것이라고 하는데, '블룸 필터'가 생각납니다.

두 HTML 문서를 비교하는 가장 간단한 방법은 두 문서를 문자열로 보고 비교하는 것이겠지만, 비교 대상 문서의 수가 방대한 경우(10억개)에는
느리고 비효율적이어서 적용하기 곤란할 것입니다. 

효과적인 방법은 웹 페이지의 해시 값을 비교하는 것입니다.
> 책에서는 참고문헌를 보라고 나와있어서, 어떤 방법인 지 간략하게나마 적어보겠습니다.
> 1. "abcd" 라는 문자열에 대해 문자 하나하나를 정수로 변환
> 2. 수열 완성: [97, 98, 99, 100]
> 3. 각 숫자를 다항식의 계수로 사용한다.
> 4. 97 + 98x + 99x^2 + 100x^3
> 5. 정수 x를 하나 정하고, 계산 후, 소수 p로 나는다.
> 6. y = (97 + 98x + 99x^2 + 100x^3) mod p
> 7. y값이 지문(fingerprint)가 된다.
> 
> 두 HTML 문서에 대해 지문을 비교한다.
> 
> 여기서 생기는 의문점은, '두 문서가 100% 일치할 때만 감지할 수 있는 것 아닐까?' 라는 생각이 들었습니다.
> 그래서 문서를 잘게 쪼개서 여러개의 지문을 만들어서 비교하는 방법도 존재한다고 합니다.
> 
> ❗️ 구글 검색 엔진의 경우, 단순히 단어가 비슷한지의 여부를 넘어, 콘텐츠의 핵심 정보와 문장 구조와 의미까지 비교한다고 합니다.

### 콘텐츠 저장소
콘텐츠 저장소는 HTML 문서를 보관하는 시스템입니다. 저장소를 구현하는 데 쓰일 기술을 고를 때는 저장할 데이터의 유형,
크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려해야합니다.
> 책의 설계안같은 경우에는 디스크와 메모리를 동시에 사용하는 저장소를 택했다고 합니다.
> - 데이터 양이 너무 많으므로 대부분의 콘텐츠는 디스크에 저장합니다.
> - 인기 있는 콘텐츠는 메모리에 두어 접근 지연시간을 줄입니다.

### URL 추출기
URL 추출기는 HTML 페이지를 파싱하여 링크들을 골라내는 역할을 합니다. 
아래 그림은 링크 추출 사례입니다.
![image_9-3_2.png](image%2Fimage_9-3_2.png)
그림을 보시면 아시겠지만, 콘텐츠에 있는 링크들의 상대 경로는 모두 `https://en.wikipedia.org`를 붙여 절대 경로로 변환합니다.

### URL 필터
URL 필터는 아래와 같은 URL들을 크롤링 대상에서 배제하는 역할을 합니다.
- 특정한 콘텐츠를 갖는 URL
- 특정한 파일 확장자를 갖는 URL
- 접속 시 오류가 발생하는 URL
- 접근 제외 목록에 포함된 URL

### 이미 방문한 URL?
이미 방문한 URL임을 알기 위해 'URL 저장소' 나 '미수집 URL 저장소'에 보관된 URL을 추적할 수 있도록 하는 자료 구조를 사용합니다.

이미 방문한 URL 인지 판별하게 되면, 같은 URL을 여러 번 처리하는 일을 방지할 수 있으므로 서버 부하를 줄이고, 시스템이 무한 루프에 빠지는 일을 방지할 수 있습니다.

이미 방문한 URL임을 검증하기 위한 자료구조로는 `블룸 필터`나 `해시 테이블`이 널리 쓰입니다.

### URL 저장소
URL 저장소는 `이미 방문한 URL을 보관하는 저장소` 입니다.

