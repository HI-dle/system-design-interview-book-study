# 웹 크롤러 설계

---

 - 이번 장은 웹 크롤러(web crawler)의 설계이다.
 - 고전 적인 시스템 설계 문제로 웹 크롤러나느 로봇(robot) 또는 스파이더(spider)라고도 부른다. 검색 엔진에 널리 쓰는 기술로 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적이다.
 - 이때 콘텐츠는 웹 페이지일 수 있고 이미지, 비디오, PDF 파일일 수 있다.
 - 웹 크롤러는 몇 개 웹 페이지에서 시작해 링크를 따라가며 새로운 콘텐츠를 수집한다.

![image_9-1.png](image%2Fimage_9-1.png)

- 위 그림은 시각적 예제로 정리한 내용이다. 각 a,b,c 웹 페이지에서 각 링크를 따라가 수집하고 각 페이지에서 또 파생되는 페이지를 수집하는 형식이다.

---

- 크롤러는 다양하게 이용 된다.
  - 검색 엔진 인덱싱: 크롤러의 가장 보편적인 용례로 크롤러는 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다. 일례로 googlebot은 구글은 검색 엔진이 사용하는 웹 크롤러이다.
  - 웹 아카이빙 : 나중에 사용할 목적으로 장기 보관하기 위해 웹에서 정보를 모으는 절차를 말한다. 국립 도서관이 크롤러를 돌려 웹 사이트를 아카이빙하고 있다. 대표적으로 미국 국회 도서관과 EU 웹 아카이브가 있다.
  - 웹 마이닝 : 웹의 폭발적 성장세는 데이터 마이닝 업계에 전례 없는 기회로 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출해 낼 수 있는 것이다. 유명 금융 기업들은 크롤러를 사용해 주주 총회 자료나 연차 보고서를 다운 받아 기업의 핵심 사업 방향을 알아내기도 한다.
  - 웹 모니터링 : 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다. 디지마크사는 웹 크롤러를 사용해 해적판 저장물을 찾아내 보고한다.

- 웹 크롤러의 복잡도는 웹 크롤러가 처리해야 하는 데이터의 규모에 따라 달라진다. 몇 시간이면 끝낼 수 있는 작은 학급 프로젝트 수준일 수 있고 별도의 엔지니어링 팀을 꾸려 지속적으로 유지보수 하는 대형 프로젝트가 될 수 있다.
- 따라서 웹 크롤러의 설계시 감당해야 하는 데이터의 규모와 기능을 알아내야 한다.

---

- 웹 크롤러의 설계에서 1단계를 확인해보자.

### 1단계 문제 이해 및 설계 범위 확정

 - 웹 크롤러의 기본 알고리즘은 다음과 같다.
   - 1. URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 모든 웹 페이지를 다운로드 한다.
   - 2. 다운 받은 웹 페이지에서 URL을 추출한다.
   - 3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위 과정을 처음부터 반복한다.

- 하지만 웹 크롤러는 위와 같이 단순히 동작하지 않는다. 엄청난 규모 확장성을 갖는 웹 크롤러를 설계하는 것은 엄청나게 어려운 일이다.
- 웹 크롤러에 대해 질문을 던져 요구 사항을 알아내고 설계 범위를 좁혀야 한다.
  - 크롤러에 대한 주된 용도 알아내기(검색 엔진 인덱스, 데이터 마이닝 등등)
  - 매달 어느 정도의 웹 페이지를 수집해야 하는가?(10억 개)
  - 새로 만들어진 웹 페이지나 수정된 웹 페이지를 고려해야 하는가?
  - 수집한 웹 페이지는 저장해야 하는가? 저장한다면 얼마나 저장해야 하는가?(5년)
  - 중복된 콘텐츠는 어떻게 해야 하는가?

- 위 질문들은 몇가지 사례일 뿐이고 이와 같은 질문을 통해 요구 사항을 알아내고 모호한 부분을 제거해야 한다.
- 좋은 웹 크롤러를 만족하려면 다음과 같은 속성에 주의해야 한다.
  - 규모 확장성 : 거대한 웹에서 수십억 개의 페이지가 존재하고 병행성을 활용하면 보다 효과적으로 크롤링을 할 수 있다.
    - 혼자서 1억 개의 페이지를 1초당 1개 속도로 수집하면 3년 걸린다. 하지만 1,000개의 크롤러를 동시에 돌리면 단 1.5일이면 끝난다.
  - 안정성 : 웹은 함정으로 가득한데 잘못 작성된 HTML이나 아무 반응이 없는 서버, 장애, 악성 코드가 붙어 있는 링크들이 있고 크롤러는 이런 비정상적 입력이나 환경에 잘 대응해야 한다.
    - timeout설정, redirect 횟수 제한, JS 비활성화 등으로  안정성을 챙긴다.
  - 예절 : 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내선 안 된다.
    - 뒤에 좀 더 자세히 나오니 스킵 하겠습니다.
  - 확장성 : 새로운 형태의 콘텐츠를 지원하기 쉬워야 한다. 예시로 이미지 파일도 크롤링 한다면 이미지 파일을 크롤링 하기 위해 전체 시스탬을 새로 설계해야 한다면 곤란하기 때문이다.

---

### 개략적 규모 추정

 - 이 추정치는 위 질문을 던진 가정으로부터 나온 것이다.
 - 매달 10억 개의 웹 페이지를 다운로드 한다.
 - QPS = 10억 (1,000,000,000) /30일/24시간/3600초 = 대략 400페이지/초
 - 최대(Peak) QPS = 2 X QPS = 800
 - 웹 페이지 크기 평균은 500k라 가정
 - 10억 X 500k = 500TB/월 (500,000,000,000kb가 되는데 간단히 0이 3개마다 단위가 올라간다 보면 된다. kb -> mb -> gb -> tb 이므로 500tb이다.)
 - 이로써 1개월치 데이터를 보관 하는데 500TB이고 5년간 보관한다 가정하면 500TB X 12개월 X 5년 30,000TB 이므로 30PB가 된다.
